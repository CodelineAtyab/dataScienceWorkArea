{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 130ms/step - accuracy: 0.4989 - loss: 0.7269 - val_accuracy: 0.5016 - val_loss: 0.6964\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 130ms/step - accuracy: 0.5171 - loss: 0.6988 - val_accuracy: 0.6116 - val_loss: 0.6527\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 129ms/step - accuracy: 0.6604 - loss: 0.6173 - val_accuracy: 0.7098 - val_loss: 0.5891\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 127ms/step - accuracy: 0.7000 - loss: 0.5839 - val_accuracy: 0.7500 - val_loss: 0.5561\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 140ms/step - accuracy: 0.5593 - loss: 0.6785 - val_accuracy: 0.6702 - val_loss: 0.6011\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 127ms/step - accuracy: 0.6637 - loss: 0.6038 - val_accuracy: 0.6766 - val_loss: 0.6036\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 130ms/step - accuracy: 0.7442 - loss: 0.5276 - val_accuracy: 0.7564 - val_loss: 0.5162\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 128ms/step - accuracy: 0.8027 - loss: 0.4556 - val_accuracy: 0.7776 - val_loss: 0.4975\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 131ms/step - accuracy: 0.8151 - loss: 0.4335 - val_accuracy: 0.7702 - val_loss: 0.5093\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 131ms/step - accuracy: 0.8402 - loss: 0.3932 - val_accuracy: 0.7754 - val_loss: 0.5256\n",
      "\n",
      "Evaluating RNN Model:\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - accuracy: 0.7789 - loss: 0.5187\n",
      "Test Loss: 0.5281, Test Accuracy: 0.7716\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN, LSTM, GRU, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths\n",
    "train_pos_dir = \"train/pos/\"\n",
    "train_neg_dir = \"train/neg/\"\n",
    "test_pos_dir = \"test/pos/\"\n",
    "test_neg_dir = \"test/neg/\"\n",
    "\n",
    "# Function to load data\n",
    "def load_data(directory):\n",
    "    texts, labels = [], []\n",
    "    for file in os.listdir(directory):\n",
    "        with open(os.path.join(directory, file), 'r', encoding='utf-8') as f:\n",
    "            texts.append(f.read())\n",
    "        labels.append(1 if \"pos\" in directory else 0)\n",
    "    return texts, labels\n",
    "\n",
    "# Load all data\n",
    "train_texts_pos, train_labels_pos = load_data(train_pos_dir)\n",
    "train_texts_neg, train_labels_neg = load_data(train_neg_dir)\n",
    "test_texts_pos, test_labels_pos = load_data(test_pos_dir)\n",
    "test_texts_neg, test_labels_neg = load_data(test_neg_dir)\n",
    "\n",
    "# Combine data\n",
    "texts = train_texts_pos + train_texts_neg + test_texts_pos + test_texts_neg\n",
    "labels = train_labels_pos + train_labels_neg + test_labels_pos + test_labels_neg\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=20000, oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=300)\n",
    "\n",
    "# Encode labels to one-hot\n",
    "labels = to_categorical(labels, num_classes=2)\n",
    "\n",
    "# Split data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Model Parameters\n",
    "vocab_size = 20000\n",
    "embedding_dim = 128\n",
    "max_length = 300\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# Define RNN Model\n",
    "def build_rnn_model():\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        SimpleRNN(128, return_sequences=False),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define LSTM Model\n",
    "def build_lstm_model():\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        LSTM(128, return_sequences=False),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define GRU Model\n",
    "def build_gru_model():\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        GRU(128, return_sequences=False),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Build Models\n",
    "rnn_model = build_rnn_model()\n",
    "lstm_model = build_lstm_model()\n",
    "gru_model = build_gru_model()\n",
    "\n",
    "# Train RNN Model\n",
    "rnn_history = rnn_model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=epochs)\n",
    "# Evaluate Models\n",
    "print(\"\\nEvaluating RNN Model:\")\n",
    "rnn_eval = rnn_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {rnn_eval[0]:.4f}, Test Accuracy: {rnn_eval[1]:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 382ms/step - accuracy: 0.7150 - loss: 0.5373 - val_accuracy: 0.8466 - val_loss: 0.3653\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 380ms/step - accuracy: 0.8523 - loss: 0.3464 - val_accuracy: 0.8754 - val_loss: 0.2935\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 424ms/step - accuracy: 0.9276 - loss: 0.1959 - val_accuracy: 0.8710 - val_loss: 0.3059\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 376ms/step - accuracy: 0.9568 - loss: 0.1237 - val_accuracy: 0.8906 - val_loss: 0.2989\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 384ms/step - accuracy: 0.9719 - loss: 0.0858 - val_accuracy: 0.8864 - val_loss: 0.3636\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 360ms/step - accuracy: 0.9740 - loss: 0.0748 - val_accuracy: 0.8798 - val_loss: 0.3419\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 377ms/step - accuracy: 0.9707 - loss: 0.0824 - val_accuracy: 0.8882 - val_loss: 0.4503\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 385ms/step - accuracy: 0.9897 - loss: 0.0358 - val_accuracy: 0.8862 - val_loss: 0.4951\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 376ms/step - accuracy: 0.9901 - loss: 0.0304 - val_accuracy: 0.8836 - val_loss: 0.5451\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 375ms/step - accuracy: 0.9919 - loss: 0.0279 - val_accuracy: 0.8772 - val_loss: 0.5501\n",
      "\n",
      "Evaluating LSTM Model:\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 76ms/step - accuracy: 0.8924 - loss: 0.4864\n",
      "Test Loss: 0.5360, Test Accuracy: 0.8824\n"
     ]
    }
   ],
   "source": [
    "# Train LSTM Model\n",
    "lstm_history = lstm_model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=epochs)\n",
    "# Evaluate Models\n",
    "print(\"\\nEvaluating LSTM Model:\")\n",
    "lstm_eval = lstm_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {lstm_eval[0]:.4f}, Test Accuracy: {lstm_eval[1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m248s\u001b[0m 392ms/step - accuracy: 0.6962 - loss: 0.5387 - val_accuracy: 0.8716 - val_loss: 0.3155\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 387ms/step - accuracy: 0.9202 - loss: 0.2138 - val_accuracy: 0.9050 - val_loss: 0.2397\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 384ms/step - accuracy: 0.9624 - loss: 0.1108 - val_accuracy: 0.9024 - val_loss: 0.2895\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 393ms/step - accuracy: 0.9821 - loss: 0.0556 - val_accuracy: 0.8970 - val_loss: 0.3303\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m248s\u001b[0m 397ms/step - accuracy: 0.9893 - loss: 0.0356 - val_accuracy: 0.8948 - val_loss: 0.4316\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 387ms/step - accuracy: 0.9943 - loss: 0.0206 - val_accuracy: 0.8934 - val_loss: 0.5092\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 386ms/step - accuracy: 0.9955 - loss: 0.0145 - val_accuracy: 0.8916 - val_loss: 0.5973\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m249s\u001b[0m 398ms/step - accuracy: 0.9966 - loss: 0.0109 - val_accuracy: 0.8908 - val_loss: 0.6214\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 408ms/step - accuracy: 0.9968 - loss: 0.0092 - val_accuracy: 0.8888 - val_loss: 0.6647\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 387ms/step - accuracy: 0.9986 - loss: 0.0049 - val_accuracy: 0.8918 - val_loss: 0.7567\n",
      "\n",
      "Evaluating GRU Model:\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 71ms/step - accuracy: 0.8997 - loss: 0.6717\n",
      "Test Loss: 0.7593, Test Accuracy: 0.8922\n"
     ]
    }
   ],
   "source": [
    "# Train GRU Model\n",
    "gru_history = gru_model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=epochs)\n",
    "# Evaluate Models\n",
    "print(\"\\nEvaluating GRU Model:\")\n",
    "gru_eval = gru_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {gru_eval[0]:.4f}, Test Accuracy: {gru_eval[1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
